# Transformer Implementation 

This is my personal implementation of the Transformer architecture from the 'Attention is All You Need' paper. I've made it modular using object oriented programming for better readability and reusability. I am currently working on also making this code confugurable to train a decoder only GPT. The code currently only works for Encoder-Decoder training on translation data. 